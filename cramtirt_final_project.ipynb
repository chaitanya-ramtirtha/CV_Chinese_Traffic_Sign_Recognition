{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c2a4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import PIL.Image as Image\n",
    "from keras.applications.mobilenet_v2 import  preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd53ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('.\\\\chinese-traffic-signs\\\\annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc80553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000_0001.png</td>\n",
       "      <td>134</td>\n",
       "      <td>128</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000_0002.png</td>\n",
       "      <td>165</td>\n",
       "      <td>151</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>149</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000_0003.png</td>\n",
       "      <td>128</td>\n",
       "      <td>122</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>116</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000_0010.png</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000_0011.png</td>\n",
       "      <td>186</td>\n",
       "      <td>174</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name  width  height  x1  y1   x2   y2  category\n",
       "0  000_0001.png    134     128  19   7  120  117         0\n",
       "1  000_0002.png    165     151  23  12  149  138         0\n",
       "2  000_0003.png    128     122  22  14  116  105         0\n",
       "3  000_0010.png     80      73  14   8   67   63         0\n",
       "4  000_0011.png    186     174  36  15  155  157         0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac38fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "train_df['category'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d296416",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category'] = train_df['category'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b873f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import  preprocess_input\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            \n",
    "            preprocessing_function=preprocess_input,\n",
    "            validation_split=0.1,\n",
    "            featurewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3872275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5548 validated image filenames belonging to 58 classes.\n",
      "Train generator created\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory=\".\\\\chinese-traffic-signs\\images\",\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"category\",\n",
    "            has_ext=False,\n",
    "            subset=\"training\",\n",
    "            batch_size=16,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=(224,244))\n",
    "print('Train generator created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a5d00ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=list(train_generator.class_indices.keys())\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5101d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import  preprocess_input\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            \n",
    "            preprocessing_function=preprocess_input,\n",
    "            validation_split=0.1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7899ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 616 validated image filenames belonging to 58 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory='.\\\\chinese-traffic-signs\\images',\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"category\",\n",
    "            has_ext=False,\n",
    "            subset=\"validation\",\n",
    "            batch_size=16,\n",
    "            seed=42,\n",
    "            shuffle=False,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb563ef",
   "metadata": {},
   "source": [
    "### MobileVNet2 model(pre-trained model) \n",
    "Source = https://www.kaggle.com/code/hossamfakher/traffic-sign-classification-acc-96/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ac5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense ,Flatten ,Conv2D ,MaxPooling2D ,Dropout ,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import EarlyStopping ,ReduceLROnPlateau ,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8907a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5250e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStop=EarlyStopping(patience=10,restore_best_weights=True)\n",
    "Reduce_LR=ReduceLROnPlateau(monitor='val_accuracy',verbose=2,factor=0.5,min_lr=0.00001)\n",
    "model_check=ModelCheckpoint('Trafic_sign.hdf5',monitor='val_loss',verbose=1,save_best_only=True)\n",
    "callback=[EarlyStop , Reduce_LR,model_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5530cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "mb=MobileNetV2(include_top=False,input_shape=(224,224,3),weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b774711",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168f1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(mb)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1024 , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 58 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db2f92dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 62720)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               32113152  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 58)                29754     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,459,194\n",
      "Trainable params: 33,197,114\n",
      "Non-trainable params: 2,262,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a39bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3648182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.2723 - accuracy: 0.3244\n",
      "Epoch 1: val_loss improved from inf to 3.25989, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 31s 1s/step - loss: 3.2723 - accuracy: 0.3244 - val_loss: 3.2599 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.3779 - accuracy: 0.4494\n",
      "Epoch 2: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 22s 1s/step - loss: 2.3779 - accuracy: 0.4494 - val_loss: 8.2968 - val_accuracy: 0.0312 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8779 - accuracy: 0.5327\n",
      "Epoch 3: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 23s 1s/step - loss: 1.8779 - accuracy: 0.5327 - val_loss: 6.4637 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6922 - accuracy: 0.5833\n",
      "Epoch 4: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.6922 - accuracy: 0.5833 - val_loss: 3.7622 - val_accuracy: 0.0312 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4413 - accuracy: 0.6280\n",
      "Epoch 5: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 24s 1s/step - loss: 1.4413 - accuracy: 0.6280 - val_loss: 6.3629 - val_accuracy: 0.0625 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3625 - accuracy: 0.6131\n",
      "Epoch 6: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.3625 - accuracy: 0.6131 - val_loss: 4.4872 - val_accuracy: 0.0625 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2173 - accuracy: 0.6488\n",
      "Epoch 7: val_loss did not improve from 3.25989\n",
      "21/21 [==============================] - 24s 1s/step - loss: 1.2173 - accuracy: 0.6488 - val_loss: 4.3122 - val_accuracy: 0.0938 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1417 - accuracy: 0.6786\n",
      "Epoch 8: val_loss improved from 3.25989 to 2.22790, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 28s 1s/step - loss: 1.1417 - accuracy: 0.6786 - val_loss: 2.2279 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1019 - accuracy: 0.6815\n",
      "Epoch 9: val_loss improved from 2.22790 to 0.91749, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 19s 932ms/step - loss: 1.1019 - accuracy: 0.6815 - val_loss: 0.9175 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0026 - accuracy: 0.7113\n",
      "Epoch 10: val_loss did not improve from 0.91749\n",
      "21/21 [==============================] - 16s 763ms/step - loss: 1.0026 - accuracy: 0.7113 - val_loss: 1.1303 - val_accuracy: 0.6562 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8957 - accuracy: 0.7440\n",
      "Epoch 11: val_loss did not improve from 0.91749\n",
      "21/21 [==============================] - 18s 836ms/step - loss: 0.8957 - accuracy: 0.7440 - val_loss: 1.9154 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9555 - accuracy: 0.7530\n",
      "Epoch 12: val_loss improved from 0.91749 to 0.70028, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 33s 2s/step - loss: 0.9555 - accuracy: 0.7530 - val_loss: 0.7003 - val_accuracy: 0.8438 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.7083\n",
      "Epoch 13: val_loss improved from 0.70028 to 0.38481, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 36s 2s/step - loss: 1.0482 - accuracy: 0.7083 - val_loss: 0.3848 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.7351\n",
      "Epoch 14: val_loss did not improve from 0.38481\n",
      "21/21 [==============================] - 31s 1s/step - loss: 0.9174 - accuracy: 0.7351 - val_loss: 0.7478 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8400 - accuracy: 0.7440\n",
      "Epoch 15: val_loss did not improve from 0.38481\n",
      "21/21 [==============================] - 34s 2s/step - loss: 0.8400 - accuracy: 0.7440 - val_loss: 0.4303 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8141 - accuracy: 0.7411\n",
      "Epoch 16: val_loss did not improve from 0.38481\n",
      "21/21 [==============================] - 32s 2s/step - loss: 0.8141 - accuracy: 0.7411 - val_loss: 0.8293 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7671 - accuracy: 0.7827\n",
      "Epoch 17: val_loss did not improve from 0.38481\n",
      "21/21 [==============================] - 30s 1s/step - loss: 0.7671 - accuracy: 0.7827 - val_loss: 2.3316 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.7946\n",
      "Epoch 18: val_loss improved from 0.38481 to 0.29484, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 29s 1s/step - loss: 0.7424 - accuracy: 0.7946 - val_loss: 0.2948 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8615 - accuracy: 0.7440\n",
      "Epoch 19: val_loss did not improve from 0.29484\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.8615 - accuracy: 0.7440 - val_loss: 0.6934 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.8185\n",
      "Epoch 20: val_loss improved from 0.29484 to 0.20611, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 32s 2s/step - loss: 0.5902 - accuracy: 0.8185 - val_loss: 0.2061 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.7798\n",
      "Epoch 21: val_loss improved from 0.20611 to 0.20188, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 28s 1s/step - loss: 0.6960 - accuracy: 0.7798 - val_loss: 0.2019 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.8214\n",
      "Epoch 22: val_loss did not improve from 0.20188\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.6373 - accuracy: 0.8214 - val_loss: 0.2786 - val_accuracy: 0.9062 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.8095\n",
      "Epoch 23: val_loss did not improve from 0.20188\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.6463 - accuracy: 0.8095 - val_loss: 0.5155 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.8036\n",
      "Epoch 24: val_loss did not improve from 0.20188\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.6211 - accuracy: 0.8036 - val_loss: 0.4111 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.7827\n",
      "Epoch 25: val_loss did not improve from 0.20188\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.6517 - accuracy: 0.7827 - val_loss: 0.2354 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6323 - accuracy: 0.8274\n",
      "Epoch 26: val_loss improved from 0.20188 to 0.19084, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 29s 1s/step - loss: 0.6323 - accuracy: 0.8274 - val_loss: 0.1908 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.8393\n",
      "Epoch 27: val_loss did not improve from 0.19084\n",
      "21/21 [==============================] - 22s 1s/step - loss: 0.5306 - accuracy: 0.8393 - val_loss: 0.3399 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.7976\n",
      "Epoch 28: val_loss did not improve from 0.19084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 24s 1s/step - loss: 0.6447 - accuracy: 0.7976 - val_loss: 0.4486 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8393\n",
      "Epoch 29: val_loss improved from 0.19084 to 0.17794, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 31s 1s/step - loss: 0.5608 - accuracy: 0.8393 - val_loss: 0.1779 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.8512\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.17794\n",
      "21/21 [==============================] - 18s 778ms/step - loss: 0.5136 - accuracy: 0.8512 - val_loss: 0.3638 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.8393\n",
      "Epoch 31: val_loss improved from 0.17794 to 0.16259, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 21s 1s/step - loss: 0.5356 - accuracy: 0.8393 - val_loss: 0.1626 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8661\n",
      "Epoch 32: val_loss improved from 0.16259 to 0.14970, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 21s 1s/step - loss: 0.3764 - accuracy: 0.8661 - val_loss: 0.1497 - val_accuracy: 0.8750 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8869\n",
      "Epoch 33: val_loss improved from 0.14970 to 0.13235, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.3526 - accuracy: 0.8869 - val_loss: 0.1324 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.8780\n",
      "Epoch 34: val_loss did not improve from 0.13235\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.4018 - accuracy: 0.8780 - val_loss: 0.1694 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8958\n",
      "Epoch 35: val_loss did not improve from 0.13235\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.3480 - accuracy: 0.8958 - val_loss: 0.2013 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.8810\n",
      "Epoch 36: val_loss did not improve from 0.13235\n",
      "21/21 [==============================] - 14s 651ms/step - loss: 0.3885 - accuracy: 0.8810 - val_loss: 0.1841 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8886\n",
      "Epoch 37: val_loss improved from 0.13235 to 0.12244, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 16s 750ms/step - loss: 0.3526 - accuracy: 0.8886 - val_loss: 0.1224 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9137\n",
      "Epoch 38: val_loss improved from 0.12244 to 0.05906, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.3051 - accuracy: 0.9137 - val_loss: 0.0591 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.8720\n",
      "Epoch 39: val_loss improved from 0.05906 to 0.03512, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.3882 - accuracy: 0.8720 - val_loss: 0.0351 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8988\n",
      "Epoch 40: val_loss did not improve from 0.03512\n",
      "21/21 [==============================] - 20s 910ms/step - loss: 0.3301 - accuracy: 0.8988 - val_loss: 0.1058 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.8601\n",
      "Epoch 41: val_loss improved from 0.03512 to 0.01736, saving model to Trafic_sign.hdf5\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.4464 - accuracy: 0.8601 - val_loss: 0.0174 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.9048\n",
      "Epoch 42: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 22s 1s/step - loss: 0.2945 - accuracy: 0.9048 - val_loss: 0.0236 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8988\n",
      "Epoch 43: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 21s 994ms/step - loss: 0.2964 - accuracy: 0.8988 - val_loss: 0.0265 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.8869\n",
      "Epoch 44: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.3308 - accuracy: 0.8869 - val_loss: 0.0331 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8988\n",
      "Epoch 45: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 19s 857ms/step - loss: 0.3102 - accuracy: 0.8988 - val_loss: 0.0417 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.9107\n",
      "Epoch 46: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 20s 979ms/step - loss: 0.3045 - accuracy: 0.9107 - val_loss: 0.0592 - val_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.9048\n",
      "Epoch 47: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.3155 - accuracy: 0.9048 - val_loss: 0.1235 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8958\n",
      "Epoch 48: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 17s 787ms/step - loss: 0.3404 - accuracy: 0.8958 - val_loss: 0.1358 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9196\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 15s 707ms/step - loss: 0.2321 - accuracy: 0.9196 - val_loss: 0.1173 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9226\n",
      "Epoch 50: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 16s 775ms/step - loss: 0.2201 - accuracy: 0.9226 - val_loss: 0.1101 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9405\n",
      "Epoch 51: val_loss did not improve from 0.01736\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.2012 - accuracy: 0.9405 - val_loss: 0.0633 - val_accuracy: 0.9688 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_generator,validation_data=val_generator,epochs=100,batch_size=16,\n",
    "                  steps_per_epoch=len(train_generator)//16,validation_steps=len(val_generator)//16,\n",
    "                  callbacks=callback, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "993f61a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 25s 640ms/step - loss: 0.4921 - accuracy: 0.8474\n"
     ]
    }
   ],
   "source": [
    "loss,acc=model.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79291b7f",
   "metadata": {},
   "source": [
    "#### The MobileVNet model is not unique. It is inspired from kaggle notebook.\n",
    "#### I'm going to define a CNN model which train from scratch will classify the traffic signs into 58 categories. I'll try different parameters and layers to see if the model performance and accuracy is better compared to MobileVNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "124be3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(58, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e8df3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa42f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "346/346 [==============================] - 443s 1s/step - loss: 0.5127 - accuracy: 0.8261 - val_loss: 0.5518 - val_accuracy: 0.7993\n",
      "Epoch 2/20\n",
      "346/346 [==============================] - 417s 1s/step - loss: 0.4366 - accuracy: 0.8529 - val_loss: 0.4108 - val_accuracy: 0.8586\n",
      "Epoch 3/20\n",
      "346/346 [==============================] - 400s 1s/step - loss: 0.4305 - accuracy: 0.8563 - val_loss: 0.4310 - val_accuracy: 0.8635\n",
      "Epoch 4/20\n",
      "346/346 [==============================] - 283s 817ms/step - loss: 0.3665 - accuracy: 0.8736 - val_loss: 0.4279 - val_accuracy: 0.8668\n",
      "Epoch 5/20\n",
      "346/346 [==============================] - 370s 1s/step - loss: 0.3597 - accuracy: 0.8832 - val_loss: 0.3764 - val_accuracy: 0.8536\n",
      "Epoch 6/20\n",
      "346/346 [==============================] - 353s 1s/step - loss: 0.2979 - accuracy: 0.8973 - val_loss: 0.2687 - val_accuracy: 0.9112\n",
      "Epoch 7/20\n",
      "346/346 [==============================] - 338s 975ms/step - loss: 0.2980 - accuracy: 0.9024 - val_loss: 0.2114 - val_accuracy: 0.9293\n",
      "Epoch 8/20\n",
      "346/346 [==============================] - 347s 1s/step - loss: 0.2745 - accuracy: 0.9087 - val_loss: 0.3637 - val_accuracy: 0.8799\n",
      "Epoch 9/20\n",
      "346/346 [==============================] - 375s 1s/step - loss: 0.2698 - accuracy: 0.9037 - val_loss: 0.2500 - val_accuracy: 0.9128\n",
      "Epoch 10/20\n",
      "346/346 [==============================] - 409s 1s/step - loss: 0.2525 - accuracy: 0.9152 - val_loss: 0.3086 - val_accuracy: 0.9079\n",
      "Epoch 11/20\n",
      "346/346 [==============================] - 395s 1s/step - loss: 0.2100 - accuracy: 0.9290 - val_loss: 0.2456 - val_accuracy: 0.9194\n",
      "Epoch 12/20\n",
      "346/346 [==============================] - 375s 1s/step - loss: 0.2253 - accuracy: 0.9264 - val_loss: 0.1849 - val_accuracy: 0.9375\n",
      "Epoch 13/20\n",
      "346/346 [==============================] - 296s 854ms/step - loss: 0.2071 - accuracy: 0.9324 - val_loss: 0.1250 - val_accuracy: 0.9523\n",
      "Epoch 14/20\n",
      "346/346 [==============================] - 231s 668ms/step - loss: 0.1871 - accuracy: 0.9380 - val_loss: 0.1692 - val_accuracy: 0.9441\n",
      "Epoch 15/20\n",
      "346/346 [==============================] - 215s 620ms/step - loss: 0.1720 - accuracy: 0.9438 - val_loss: 0.1054 - val_accuracy: 0.9622\n",
      "Epoch 16/20\n",
      "346/346 [==============================] - 229s 663ms/step - loss: 0.2241 - accuracy: 0.9309 - val_loss: 0.1168 - val_accuracy: 0.9605\n",
      "Epoch 17/20\n",
      "346/346 [==============================] - 272s 785ms/step - loss: 0.1659 - accuracy: 0.9431 - val_loss: 0.1292 - val_accuracy: 0.9474\n",
      "Epoch 18/20\n",
      "346/346 [==============================] - 302s 873ms/step - loss: 0.1767 - accuracy: 0.9429 - val_loss: 0.0842 - val_accuracy: 0.9737\n",
      "Epoch 19/20\n",
      "346/346 [==============================] - 306s 883ms/step - loss: 0.1373 - accuracy: 0.9563 - val_loss: 0.1395 - val_accuracy: 0.9507\n",
      "Epoch 20/20\n",
      "346/346 [==============================] - 305s 881ms/step - loss: 0.1447 - accuracy: 0.9516 - val_loss: 0.0527 - val_accuracy: 0.9803\n",
      "347/347 - 181s - loss: 0.0431 - accuracy: 0.9858 - 181s/epoch - 521ms/step\n",
      "Train accuracy: 0.9857606291770935\n",
      "39/39 - 7s - loss: 0.0520 - accuracy: 0.9805 - 7s/epoch - 174ms/step\n",
      "Test accuracy: 0.9805194735527039\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples // 16,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_generator.samples // 16)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_loss, train_acc = model_1.evaluate(train_generator, verbose=2)\n",
    "print('Train accuracy:', train_acc)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model_1.evaluate(val_generator, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8ab3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 7s 180ms/step - loss: 0.0520 - accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "loss_1,acc_1=model_1.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "740fa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save('traffic_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da4088",
   "metadata": {},
   "source": [
    "##### This model is CNN model which was uniquely designed by me. After trying different parameters and layers and multiple tries and failures, the model gives better performance compared to MobileVNet model and has better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e8dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.models.load_model('Trafic_sign.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adf768ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 13s 297ms/step - loss: 0.4921 - accuracy: 0.8474\n"
     ]
    }
   ],
   "source": [
    "loss3,acc3=model_3.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "516b9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.models.load_model('traffic_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9de1c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 4s 103ms/step - loss: 0.0520 - accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "loss4,acc4=model_2.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "571da23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MobileVNet2 model is  0.8474025726318359\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of MobileVNet2 model is \",acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37dad23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN model is  0.9805194735527039\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of CNN model is \",acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e61657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "image = Image.open('D:\\\\SUNY UB MS ES DS\\\\UB MS ES DS SEM 2\\\\CV_IP(573)\\\\Project\\\\\\Model_test_images\\\\000_1_0004_1_j.png')\n",
    "image = image.resize((224, 224))\n",
    "image = np.array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = datagen.flow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2785ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {0:\"Speed limit - 5\",1:\"Speed limit - 15\",2:\"Speed limit - 30\",3:\"Speed limit - 40\",4:\"Speed limit - 50\",5:\"Speed limit - 60\",\n",
    "                6:\"Speed limit - 70\",7:\"Speed limit - 80\",8:\"Only right turn is allowed\",9:\"Only left turn is allowed\",\n",
    "                10: \"No entry, One way\" , 11:\"No left turn allowed\"  , 12: \"No left or right turn allowed\" , 13: \"No right turn allowed\"\n",
    "                ,14: \"Cannot change lanes\" , 15:\"No U-turn allowed\" , 16: \"No thoroughfare for vehicles\",17: \"No honking\"\n",
    "                ,18: \"End speed limit 40\",19: \"End speed limit 50\",20:\"Allowed to go only right or straight\", 21: \"No turn allowed\",\n",
    "                22:\"Only left turn allowed\", 23:\"Cannot go straight\", 24:\"Only right turn allowed\", 25:\"Keep left\",\n",
    "                26:\"Keep right\", 27:\"Roundabout\", 28:\"Only cars allowed\", 29: \"Honking allowed\",\n",
    "                30:\"Cycle lane\", 31:\"U-turn allowed\",32:\"Warning for an obstacle, pass either side\", 33:\"Traffic signal warning\", \n",
    "                34:\"Warning for a danger with no specific traffic sign.\", 35:\"Warning for a crossing for pedestrians.\", 36: \"Warning for cyclists.\" ,\n",
    "                37:\"Warning for children.\", 38:\"Warning for a curve to the right.\", 39:\"Warning for a curve to the left.\"\n",
    "                ,40: \"Warning for steep descent\" ,41: \"Warning for steep ascent\", 42: \"Slow\",\n",
    "                43:\"Side road junction ahead on the right\", 44:\"Side road junction ahead on the left\",\n",
    "                45:\"Cross-village road\", 46:\"Double curve, with turn right first, then left\", \n",
    "                47:\"Locomotive railroad crossing ahead - without safety barriers\", 48:\"Roadworks ahead\",\n",
    "                49:\"Multiple curves\", 50: \"Railroad head - with safety barriers\",\n",
    "                51:\"Accident area\", 52:\"Stop\", 53:\"No entry for vehicular and pedestrians\", 54:\"No stopping\",\n",
    "                55:\"No entry for vehicular traffic\", 56:\"Give way\", 57:\"Control\"}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1857a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      " Warning -- This model is not perfect, There is a chance of mis-classification\n",
      "Speed limit - 70\n"
     ]
    }
   ],
   "source": [
    "prediction = model_2.predict(image)\n",
    "category = np.argmax(prediction)\n",
    "\n",
    "if category in category_dict.keys():\n",
    "    print(\" Warning -- This model is not perfect, There is a chance of mis-classification\")\n",
    "    print(category_dict[category])\n",
    "else:\n",
    "    print(\"Image does not belong to the pre-defined 58 categories. Please search online.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07318cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
